{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Natural Language Processing(NLP)-(Text_Preprocessing).ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS-2_mekMbNY"
      },
      "source": [
        "<h1 align=center><font size=5>Text Preprocessing</font></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZyQvbiGMbNc"
      },
      "source": [
        "### One-hot encoding <a id=\"one_hot\"></a>\n",
        "\n",
        "We will create a zero vector with length equal to the vocabulary, then place a one in the index that corresponds to the word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOCFT0K1MbNd",
        "outputId": "530d3d4d-c404-4683-ad3b-e95c8dc9fbef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "text = 'The   cat sat on  the mat.'\n",
        "text = text.lower().split()\n",
        "print(text)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(text)\n",
        "print(integer_encoded)\n",
        "\n",
        "onehot_encoded = to_categorical(integer_encoded)\n",
        "print(onehot_encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'cat', 'sat', 'on', 'the', 'mat.']\n",
            "[4 0 3 2 4 1]\n",
            "[[0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TQL81YNMbNj"
      },
      "source": [
        "> This approach is inefficient. Imagine we have 10,000 words in the vocabulary. To one-hot encode, we would create a vector where 99.99% of the elements are zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxDQ4L2QMbNk"
      },
      "source": [
        "### Encode each word with a unique number <a id=\"integer_enc\"></a>\n",
        "\n",
        "A second approach we might try is to encode each word using a unique number. Continuing the example above, we could assign 1 to \"cat\", 2 to \"mat\", and so on. We could then encode the sentence \"The cat sat on the mat\" as a dense vector like [5, 1, 4, 3, 5, 2]. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQzhTGdgMbNm"
      },
      "source": [
        "#### Text Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0afxMLiMbNo",
        "outputId": "5fce5b5b-c079-4c25-92de-5585908a498c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "texts = ['The cat sat on the mat.',\n",
        "         'The dog sat on the log.',\n",
        "         'Dogs and cats living together.']\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 20) \n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Word index:\\n', word_index)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "print('Sequences:\\n', sequences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word index:\n",
            " {'the': 1, 'sat': 2, 'on': 3, 'cat': 4, 'mat': 5, 'dog': 6, 'log': 7, 'dogs': 8, 'and': 9, 'cats': 10, 'living': 11, 'together': 12}\n",
            "Sequences:\n",
            " [[1, 4, 2, 3, 1, 5], [1, 6, 2, 3, 1, 7], [8, 9, 10, 11, 12]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_yY5GQfMbNr"
      },
      "source": [
        "#### Test Sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKwaxYghMbNr",
        "outputId": "072ae587-39f2-42a8-ce63-ad443857c22f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "X_train = ['The cat sat on the mat.']\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 20) \n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Word index:\\n', word_index)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "print('Sequences:\\n', X_train_seq)\n",
        "# --------------------------------------------------------\n",
        "X_test = ['The dog sat on the log.']\n",
        "\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "print('Test sequence:\\n', X_test_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word index:\n",
            " {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5}\n",
            "Sequences:\n",
            " [[1, 2, 3, 4, 1, 5]]\n",
            "Test sequence:\n",
            " [[1, 3, 4, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVyi8kXBMbNu"
      },
      "source": [
        "#### Out Of Vocabulary (OOV) words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKEnpSsPMbNu",
        "outputId": "7ed8013f-579a-426f-9ed4-feafa6555843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "X_train = ['The cat sat on the mat.']\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 20, oov_token = '<OOV>') \n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Word index:\\n', word_index)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "print('Sequences:\\n', X_train_seq)\n",
        "# --------------------------------------------------------\n",
        "X_test = ['The dog sat on the log.']\n",
        "\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "print('Test sequence:\\n', X_test_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word index:\n",
            " {'<OOV>': 1, 'the': 2, 'cat': 3, 'sat': 4, 'on': 5, 'mat': 6}\n",
            "Sequences:\n",
            " [[2, 3, 4, 5, 2, 6]]\n",
            "Test sequence:\n",
            " [[2, 1, 4, 5, 2, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enveOUA6MbNx"
      },
      "source": [
        "#### Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e09M2Q9IMbNx",
        "outputId": "9b0b1797-dc86-4edd-db69-c0bb094b7858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences = ['I love my dog', \n",
        "             'You love my dog!',\n",
        "             'Do you think my dog is amazing?']\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 20, oov_token = '<OOV>') \n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Word index:\\n', word_index)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print('Sequences:\\n', sequences)\n",
        "\n",
        "padded = pad_sequences(sequences)\n",
        "print('Padded sequences:\\n', padded)\n",
        "\n",
        "matrix2 = tokenizer.texts_to_matrix(['I love my dog']) \n",
        "print(matrix2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word index:\n",
            " {'<OOV>': 1, 'my': 2, 'dog': 3, 'love': 4, 'you': 5, 'i': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
            "Sequences:\n",
            " [[6, 4, 2, 3], [5, 4, 2, 3], [7, 5, 8, 2, 3, 9, 10]]\n",
            "Padded sequences:\n",
            " [[ 0  0  0  6  4  2  3]\n",
            " [ 0  0  0  5  4  2  3]\n",
            " [ 7  5  8  2  3  9 10]]\n",
            "[[0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5pBNdNJMbN0",
        "outputId": "8894dd72-8374-4715-b5c6-5648dc309f26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "padded = pad_sequences(sequences, padding = 'post', maxlen = 5, truncating = 'post') \n",
        "print('Padded sequences:\\n', padded)\n",
        "print('Padded shape:', padded.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Padded sequences:\n",
            " [[6 4 2 3 0]\n",
            " [5 4 2 3 0]\n",
            " [7 5 8 2 3]]\n",
            "Padded shape: (3, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4AM9sDcMbN2",
        "outputId": "eb854248-171e-4308-eb08-b70ba09f5759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "texts = ['The the the the the cat sat on the mat cat.']\n",
        "tokenizer = Tokenizer(num_words = 10) \n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Word index:', word_index)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "print('Sequences:', sequences)\n",
        "\n",
        "for mode in ['binary', 'count', 'freq', 'tfidf']:\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode)\n",
        "    print('-'*20, mode, '-'*20)\n",
        "    print(matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word index: {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5}\n",
            "Sequences: [[1, 1, 1, 1, 1, 2, 3, 4, 1, 5, 2]]\n",
            "-------------------- binary --------------------\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 0. 0.]]\n",
            "-------------------- count --------------------\n",
            "[[0. 6. 2. 1. 1. 1. 0. 0. 0. 0.]]\n",
            "-------------------- freq --------------------\n",
            "[[0.         0.54545455 0.18181818 0.09090909 0.09090909 0.09090909\n",
            "  0.         0.         0.         0.        ]]\n",
            "-------------------- tfidf --------------------\n",
            "[[0.         1.13196106 0.6865121  0.40546511 0.40546511 0.40546511\n",
            "  0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}